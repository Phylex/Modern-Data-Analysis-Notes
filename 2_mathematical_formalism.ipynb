{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern Methods of Data Analysis 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical formulation of Probability\n",
    "As many things in mathematics Statistics is based on set theort. Subsequently all quantities are described via Sets.\n",
    "Let's consider some hypothetical experiment. The different states of the Experiment are illustrated below.\n",
    "\n",
    "<img src=\"graphics/Probabilities.svg\">\n",
    "\n",
    "Let's also assume, that all possible outcomes of the experiment are known (the experiment may be a dice throw where there are only 6 possible outcomes if we assume it is a 6 sided dice ( and, well, if we disregard the exact position that the dice ends up in after the throw, but for most games it is irrelevant where the dice ends up (maybe under the table?) as long as the number shown can be read).\n",
    "Then $S$ is the state space of all possible outcomes of the experiment. As $A$ and $B$ lie within $S$ they are subsets of $S$.\n",
    "\n",
    "As all possible outcomes are contained in $S$ from which follows $P(S) = 1$, so in the frequentist interpretation is that the every measured quantity lies in $S$, or in bayesian interpretation the statement, \"all measurements lie within $S$\" is true.\n",
    "\n",
    "For most part, when looking at experiments, the frequentist interpretation is a bit more helpful which is why focus is placed on this interpretation for now.\n",
    "As for $A$ and $B$, as both are subsets of $S$ it can be deduced that $P(A) < P(S)$ and $P(B) < P(S)$, as both $A \\subset S$ and $B \\subset S$ (note that they are proper subsets).\n",
    "\n",
    "If it is of interest that the experiments produces a result that is either in $A$ or in $B$ and they satisfy $A \\cup B = \\emptyset$ (as shown in the illustration above) then it can be shown that $P(A\\cup B) = P(A) + P(B)$\n",
    "\n",
    "These three traits are know as the **Kolmogorov Axioms** and are written more concicely below\n",
    "\n",
    "\n",
    "$$1 \\geq P(A) \\geq 0; \\forall A \\subseteq S \\land A \\neq \\emptyset$$\n",
    "\n",
    "$$P(S) = 1$$\n",
    "\n",
    "$$P(A \\cup B) = P(A) + P(B); A \\cap B = \\emptyset$$\n",
    "\n",
    "It can also be of interest to know the probability of a measurement not being in $A$. Even if a mesurement is not in $A$ it by definition has to be in $S$. An event therefore can only be in $S \\setminus A = \\bar{A}$. The probability $P(\\bar{A}) = P(S) - P(A) = 1 - P(A)$, which is how the **NOT** definition from the previous Notebook can be derived using set theory. It trivially follows, that $P(A \\cup \\bar{A}) = P(S) = 1$ and that $P(A \\cap \\bar{A}) = 0$.\n",
    "\n",
    "As it is impossible to conduct the experiment and not produce a result it follows that $P(\\emptyset) = 0$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersecting sets and conditional probability\n",
    "Now that we have measured the States corresponding to $A$ and $B$ lets assume there are two further states $C$ and $D$ that are of interest now. The following illustration shows how experimental outcomes for $C$ and $D$ are located within $S$\n",
    "\n",
    "<img src=\"graphics/intersecting_probabilities.svg\">\n",
    "\n",
    "As you can see, The measurements for $C$ and $D$ overlap. This means that the third Kologorov Axiom no longer holds for $C$ and $D$ as $C\\cap D \\neq \\emptyset$\n",
    "If we look at $P(C \\cup D) = P(C) + P(D)$ it is possible to see, that we are counting the Area $C\\cap D$ twice. Correcting for that we get:\n",
    "\n",
    "$$ P(C\\cup D) = P(C) + P(D) - P(C\\cap D); C\\cap D \\neq \\emptyset$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Probability\n",
    "Now that we have intersecting subsets of results we can ask another interesting question. Let's assume we can tune our experimental setup to only produce results from $C$. What's the new probability of obtaining a result in $D$? This question is akin to askin: Given that we know $C$ will occurr, what is the probability to get a result in $D$? This is called conditional probability. It is written as $P(D\\mid C)$ or (I think more clearly) as $P_C(D)$ which is the probability of obtaining a result in $D$ given that $C$ occurs.\n",
    "\n",
    "The thing is, that now we have \"redefined\" what $S$ is. By tweaking the experiment we have made $C$ the new $S$. So to account for that we have to reweigh the probabilitie of $P(C \\cap D)$ to fit that $P(C) = 1$. It follows that:\n",
    "\n",
    "$$P(D\\mid C) = P_C(D) = \\frac{P(C \\cap D)}{P(C)}$$\n",
    "\n",
    "From this definition of $P(D\\mid C) = P_C(D)$ it is possible to see, that we could just as well flip the definition around and ask what $P_D(C) = P(C\\mid D)$ is.\n",
    "\n",
    "$$P(C\\mid D) = P_D(C) = \\frac{P(C\\cap D)}{P(D)}$$\n",
    "\n",
    "Looking at this one can notice that $P(C\\cap D) = P(D\\cap C)$ which is why it is allowed to keep the order of $C$ and $D$ in that part of the equation. The symetry continues with the following equation.\n",
    "\n",
    "$$P(C\\cap D) = P_D(C) \\cdot P(C) = P_C(D) \\cdot P(D)$$\n",
    "\n",
    "Knowing this we can also reformulate the definition for conditional probability:\n",
    "\n",
    "$$ P_C(D) = \\frac{P_D(C) \\cdot P(C)}{P(D)} $$\n",
    "\n",
    "This equation is one of the cornerstones of, well, the scientific method. Because it relates prediction with observation. Let me explain ...\n",
    "\n",
    "In the bayesian interpretation of probability, the probability $P(T)$ is the degree of belief, that the statement $T$ is true. Lets now assume that $T$ is not a simple statement but a theory to explain a certain phenomenon. (For this example let's use the theory of gravitation as formulated by Newton). The process of deriving the way the equation looks (in this case $F = G \\cdot \\frac{m_1 \\cdot m_2}{|\\vec{r}|^2}$ where $m_i$ are the masses of the bodies and  $|\\vec{r}|^2$ the squared distance between them) is very hard to describe and relies somewhat on experience and an odd sense of beauty inherent in certain formulations. But once the form of the equation has been guessed by theoretical physicists the next step is to determin the constants of the Law. The constant that is not obvious in this case is the gravitational constant $G$.\n",
    "\n",
    "Bayesian statistics can be used to figure out the fitting value. To be able to weigh to different proposals against each other we need to know $P_D(T)$ so the degree of belief in the theory $T$ given that the data $D$ has been observed. Without the above equation there would be no way of relating the things that can be calculated (in this case the Likelihood that the measured Data was produced by a process that is governd by the law $T$ or in this case $P_T(D)$ and the prior degree of belief $P(T)$) to what we actually want (which is $P_D(T)$, so the degree of belief after the experiment has been carried out).\n",
    "The ins and outs of calculating the likelihood will be discussed in following sections.\n",
    "\n",
    "I would also like to spend a moment on $P(T)$ for it too is not a value that can easily be deduced. The good news is that the first assumption (so $P(T)$ before any experiment has ever been conducted) will (with enough observations) converge on the \"truth\", given that the form of $T$ is in deed correct. (the later is the far larger problem and the reason that Scientists still have a job). The speed of the convergence however depends on the initial guess of $P(T)$ which is why it is beneficial not to be too overconfident in your theory (if you ever are to develop one.\n",
    "\n",
    "So in the end we have:\n",
    "\n",
    "$$ P_D(T) \\propto P_T(D) \\cdot P(T)$$\n",
    "\n",
    "where $P_D(T)$ is called the *posterior probability*, $P_T(D)$ is the *likelihood* and  $P(T)$ is the *prior probability*. It is important to note, that if multiple experiments are performed the posterior of the first experiment becomes the prior of the next. Otherwise it would be as if the first experiment would have never been carried out, as the *information* gained in the experiment is reflected in the updated prior probability of the next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
